{
  
    
        "post0": {
            "title": "A Country's Secret to Happiness",
            "content": "Introduction . I started my journey to leverage Machine Learning for trying to answer the age-old question of - what exactly makes us happy? . The World Happiness Report was recommended to be a good starting point for guaging world wise bliss. Throughout our analysis, the data points surely helped us although towards the end we were able to understand that perhaps all of the variables in this report alone will not be sufficient for us to accurately measure the happiness of a country since &quot;happiness&quot; is very relative in nature. . . There are six measurements taken per country for guaging the World Happiness Index. They consist of: . GDP per Capita - Gross Domestic Product per capita for the countries . | Family - Satisfaction Rank of Family . | Life Expectancy - Avg. expected years to live . | Freedom - Perception of freedom quantified . | Generosity - Numerical value estimated based on the perception of Generosity experienced by poll takers in their country. . | Trust/Government Corruption - A quantification of the people&#39;s perceived trust in their governments. . | Dystopia Score - Score based on comparison to hypothetically the saddest country in the world. . | Dystopia Residual - Rank of any country in a particular year. . | The Happiness Score calculated in the report is actually an average of the responses to the main life evaluation question asked in the Gallup World Poll (GWP), which uses the Cantril Ladder. . Cantril Ladder involved something called as Cantril step where they ask reponsents to think of a step with the most excellent life they can think of and with that as benchmark, score their current life. . Credits Remarks to: . Univ.Ai Professor Pavlos Protopapas | Kaggle Datasets | Aashita Kesarwani - https://www.kaggle.com/aashita/guide-to-animated-bubble-charts-using-plotly - for demonstrating beautiful ways to plot bubble charts | Jesper Sören Dramsch - https://www.kaggle.com/jesperdramsch/the-reason-we-re-happy - for demonstrating wonderful means of doing data analysis | Jamaç Eren Ay - https://www.kaggle.com/yamaerenay/world-happiness-report-preprocessed - for preparing pre processed datasets and allowing it for free use for all | Problem Statement . Given the data available per country to guage the Hapiness Index, our aim is to: . Part A - Analyze and understand which factors affect the Happiness Index Score of countries | Part B - Analyze and understand the relationship between Terror Attacks and Happiness Index | Part C - Create a Model to predict the Happiness Index of a Country | Part D - To see how much Health contributes to the Happiness Index? With the current pandemic at hand, predicting COVID-19 Cases in the coming days for countries. | Part E - Creating a Dashbord for viewing COVID-19 Predictions | Part A . To Analyze and understand which factors affect the Happiness Index Score of countries . Explaratory Data Analysis . Our objective here is to look through the datasets and perform some basic analysis to understand and guage insights. . A look into Correlation . The Spearman&#39;s Rank Correlation Coefficient is used to discover the strength of a link between two sets of data. . The Spearman rank correlation coefficient, ρ considers the ranks of the values for the two variables.ρ will always be a value between -1 and 1. . | The further away ρ is from zero, the stronger the relationship between the two variables. The sign of ρ corresponds to the direction of the relationship. If it is positive, then as one variable increases, the other tends to increase. If it is negative, then as one variable increases, the other tends to decrease. . | You use Spearman’s correlation if your data have a non-linear relationship (like an exponential relationship) or you have one or more outliers. However, Spearman’s correlation is only appropriate if the relationship between your variables is monotonic. . | . happiness_score gdp_per_capita family health freedom generosity government_trust dystopia_residual year social_support . happiness_score 1.00 | 0.80 | 0.14 | 0.77 | 0.54 | 0.13 | 0.32 | 0.23 | 0.03 | 0.24 | . gdp_per_capita 0.80 | 1.00 | 0.21 | 0.78 | 0.36 | -0.01 | 0.26 | 0.06 | -0.04 | 0.14 | . family 0.14 | 0.21 | 1.00 | -0.07 | 0.01 | 0.23 | 0.10 | 0.56 | -0.59 | -0.86 | . health 0.77 | 0.78 | -0.07 | 1.00 | 0.40 | -0.02 | 0.18 | -0.05 | 0.07 | 0.38 | . freedom 0.54 | 0.36 | 0.01 | 0.40 | 1.00 | 0.33 | 0.43 | -0.00 | 0.06 | 0.23 | . generosity 0.13 | -0.01 | 0.23 | -0.02 | 0.33 | 1.00 | 0.24 | 0.16 | -0.10 | -0.18 | . government_trust 0.32 | 0.26 | 0.10 | 0.18 | 0.43 | 0.24 | 1.00 | 0.13 | 0.02 | -0.02 | . dystopia_residual 0.23 | 0.06 | 0.56 | -0.05 | -0.00 | 0.16 | 0.13 | 1.00 | 0.09 | -0.59 | . year 0.03 | -0.04 | -0.59 | 0.07 | 0.06 | -0.10 | 0.02 | 0.09 | 1.00 | 0.43 | . social_support 0.24 | 0.14 | -0.86 | 0.38 | 0.23 | -0.18 | -0.02 | -0.59 | 0.43 | 1.00 | . Inference: From the above matrixes, it seems like Health, GDP Per Capita and freedom are the top 3 factors that correlate with happiness index. . Univariate Analysis . This type of analysis consists of use of single variable. The analysis of univariate data does not deal with causes or relationships and the main purpose of the analysis is to describe the data and find patterns that exist within it. . Bivariate Analysis . This type of analysis involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to find out the relationship among the two variables . Inference: . From the above plot, we can infer that there seems to be a: Linear Relationship: happiness_score v/s gdp_per_capita, happiness_score v/s health, happiness_score v/s freedom Non-Linear Relationship: happiness_score v/s gerosity, happiness_score v/s government_trust . Performing ANOVA test between predictors and response variable to guage how significantly it affects the scoring . Analysis of Variance is a statistical method, used to check the means of two or more groups that are significantly different from each other. It assumes Hypothesis as: H0: Means of all groups are equal. H1: At least one mean of the groups are different. . If the distributions overlap or close, then the grand mean will be similar to individual means whereas if distributions are far, the grand mean and individual means differ by larger distance. | In ANOVA, we will be checking &amp; comparing both Between-group variability to Within-group variability through f-test. | If there is no significant difference between the groups that all variances are equal, the result of ANOVA’s F-ratio will be close to 1. | . The best predictors of Happiness Index are: [&#39;gdp_per_capita&#39;, &#39;government_trust&#39;, &#39;health&#39;, &#39;family&#39;] . Two of the aspects coming out of ANOVA test belong to our correlation inference i.e GDP per capita and health. Apart from that, it seems like government trust and family also play quite a significant role in realizing the happiness score. . Looking at all countries and their ranks in Happiness Index Score . . . Inference: Clearly Norwary seems to the top country scoring in Happiness Index. It is not surprising since European Countries have better living conditions. . . . Happiness with regards to Generosity and Economy . . . Inference: The farther right side bubbless are mostly contries in the European Continents. Clearly they have better GDP Per capita. Surprisingly Europeans countries score average on Generosity(Asian countries have highest generosity) but have the most Happiness Score rankings. . Happiness with regards to Health and Economy . . . Inference: The farther right side bubbles are mostly contries in the European Continents. Clearly they have better Health score as well since they are present on top. The lowest health scores mostly consists of African and Asian countries. . Happiness with regards to Family and Economy . . . Inference: . The farther right side bubbless are mostly contries in the European Continents. Clearly they have better Family ratings. The most unsatisfied family rankings is actually mixture of mostly African, South American,Asian and a few European countries &amp; North American countries. . Happiness with regards to Govt Trust and Economy . . . Inference: . Most countries rank low on government trust giving us insights into how most of the world population doesn&#39;t necessarily trust it&#39;s governments despite the ovearching push of democracy to be adoptee. High government trust countries are Rwanda and obvious countries of Sinagpore, New Zealand, Finland. . World-wide View of Countries with regards to Generosity . . . . . . . . . . . Trend of Happiness Over Time . . . . . From the chart we can notice that the continent of Europe has a good score of GDP per capita, compared to others. Australian countries contribute the least to global GDP. . Part B . Analyze and understand the relationship between Terror Attacks and Happiness Index . Thoughts/ Motive . One of the things that intrigued us terrorism across the world. With wars and conflicts happening on a day to day basis, we really wanted to understand to what extent terrorism plays a role in happiness index. For this we combined two datasets - the Happiness Datasets and the World Terrorism dataset from Global Terrorism Database(GTD). . In our datasets, we have only took the count of terror attacks and not other information such as text based data surrounding the context of what happpened, names of the weapons used and so on since that would delve into NLP. Our future work in scope is using NLP to also analyse the datasets in order to better guage the relationship between happiness and terrorism. . Processing the Datasets . Now that we have seen EDA on Happiness Index, we were wondering what about terror attacks? Clearly the factors mentioned above are not sufficient enough to explain true happiness. So we decided to see how terror attacks combine with happiness index and to answer the question if there is a correlation present. . Below Cells take time to execute due to large dataset . Terrorism Database . iyear country_txt latitude longitude summary attacktype1_txt attacktype1 targtype1_txt targsubtype1 targsubtype1_txt corp1 target1 natlty1 natlty1_txt gname motive guncertain1 guncertain2 guncertain3 individual weaptype1_txt nkill . 0 2015.00 | Iraq | 33.30 | 44.37 | 01/03/2015: An explosive device planted on a m... | Bombing/Explosion | 1 | Private Citizens &amp; Property | 73.00 | Vehicles/Transportation | Not Applicable | Minibus | 95.00 | Iraq | Unknown | NaN | 0.00 | nan | nan | 0.00 | Explosives | 2.00 | . 1 2015.00 | Bosnia-Herzegovina | 45.18 | 15.83 | 01/01/2015: Assailants stabbed Selvedin Begano... | Armed Assault | 6 | Religious Figures/Institutions | 85.00 | Religious Figure | Unknown | Imam: Selvedin Beganovic | 28.00 | Bosnia-Herzegovina | Muslim extremists | The specific motive is unknown; however, sourc... | 0.00 | nan | nan | 0.00 | Melee | 0.00 | . 2 2015.00 | Iraq | 33.30 | 44.37 | 01/01/2015: An explosive device planted in a v... | Bombing/Explosion | 1 | Educational Institution | 48.00 | Teacher/Professor/Instructor | University of Baghdad | Lecturer | 95.00 | Iraq | Unknown | NaN | 0.00 | nan | nan | 0.00 | Explosives | 1.00 | . 3 2015.00 | Sweden | 59.86 | 17.64 | 01/01/2015: An assailant threw an explosive de... | Facility/Infrastructure Attack | 3 | Religious Figures/Institutions | 86.00 | Place of Worship | Unknown | Mosque | 198.00 | Sweden | Unknown | The specific motive is unknown; however, sourc... | 0.00 | nan | nan | 0.00 | Incendiary | 0.00 | . 4 2015.00 | Libya | 32.07 | 20.15 | 01/01/2015: Assailants attacked a Haftar milit... | Bombing/Explosion | 7 | Terrorists/Non-State Militia | 94.00 | Non-State Militia | Haftar Militia | 204 Camp | 113.00 | Libya | Shura Council of Benghazi Revolutionaries | NaN | 0.00 | nan | nan | 0.00 | Explosives | nan | . Happiness Index Database . country happiness_score gdp_per_capita family health freedom generosity government_trust dystopia_residual continent Year social_support . 0 Afghanistan | 3.79 | 0.40 | 0.58 | 0.18 | 0.11 | 0.31 | 0.06 | 2.15 | Asia | 2015 | nan | . 1 Albania | 4.64 | 1.00 | 0.80 | 0.73 | 0.38 | 0.20 | 0.04 | 1.49 | Europe | 2015 | nan | . 2 Algeria | 5.87 | 1.09 | 1.15 | 0.62 | 0.23 | 0.07 | 0.15 | 2.57 | Africa | 2015 | nan | . 3 Argentina | 6.60 | 1.19 | 1.44 | 0.70 | 0.49 | 0.11 | 0.06 | 2.61 | South America | 2015 | nan | . 4 Armenia | 4.38 | 0.90 | 1.01 | 0.64 | 0.20 | 0.08 | 0.03 | 1.52 | Asia | 2015 | nan | . Exploratory Data Analysis on Combines Dataset with Terrorism . We can see that there are some countries which go through alot of terrorist attacks . There seems to be a: Linear Relationship: happiness_score v/s gdp_per_capita, happiness_score v/s health, happiness_score v/s freedom Non-Linear Relationship: happiness_score v/s gerosity, happiness_score v/s government_trust . . . Inference: . With the data that we have, there doesn&#39;t seem to be much correlation between terror attacks and happienss index. We would need more data to come to a singificant conclusion as to how terrorism really affects the happiness index. Perhaps another factors that would allow us to further understand the happiness index would be war conditions. Countries like Syria and Palestine, are in critical war zones which would make their living condtions poor and hence affecting the happiness index. . Part C . To create a Model to Predict Happiness Index . Predicting happiness Index . The MSE value of our model is: 0.25 The R2 score of our model is : 0.819 . We used Lasso Regression with the degree of 6 to perform Polynomial Lasso Regression in order to predict the Happiness Score. . Our MSE value for Lasso Regression is 0.25 and our R2 Score is 0.82 which is pretty satisfactory. . Why did we use Lasso Regression? . We understood that Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (ergo: when only a few predictors actually influence the response). This was our case where our parameters no. was relatively small hence this seemed like the good approach to take. Ridge works well if there are many large parameters of about the same value (ergo: when most predictors impact the response). | Lasso, or Least Absolute Shrinkage and Selection Operator, is quite similar conceptually to ridge regression. It adds a penalty for non-zero coefficients. However, unlike ridge regression which penalizes sum of squared coefficients (the so-called L2 penalty), lasso penalizes the sum of their absolute values (L1 penalty). As a result, for high values of λ, many coefficients are exactly zeroed under lasso. | . The MSE value of our model is: 0.27 The R2 score of our model is : 0.81 . What did we do in MLP Regressor? . Our choice of multiple number of layers here is to depict non-linearity in the model. Multiple number of layers lead to non-linearity, but excess number of layers may lead to overfitting of the model. | Experimenting and trying out multiple combinations of layers and neurons, three layers with depicted neurons turned out to be suitable for our model. | Also, we used the default Activation Function, ReLu because of our model being a Linear Regression Model and ReLu fits the best for this problem. | . Our MSE value for MLP Regressor is 0.26 and our R2 Score is 0.82 which is pretty much the same as Lasso Regression. . Predicting Terrorist attacks . We also tried experimenting witht the variables we have from the happiness dataset to see if we can satisfactorily predict no. of terrorist attacks likely to happen. . Of course the model does not have the best performance because we understand that there are more factors that affect the outcome. . Our future work here is to get more external factors relating to what sparks terrorim attacks and create model to allow for better risk handling. . The R2 score of our model is: 0.013443721879449866 . Clearly our model is not performing well here. . Part D . To see how much Health contributes to the Happiness Index? With the current pandemic at hand, predicting COVID-19 Cases in the coming days for countries. . Thoughts . From Part A, we have realized that Health does play a major role in a country&#39;s happiness score. With the current pandemic at hand, we were motivated to look at COVID cases and forecast the upcoming cases. We wanted to compare the COVID data with the happiness index data, however, we felt that it would not give the right results since the happiness index data of 2020 is from the months of January-February when there was not much COVID health crisis happening. . However, in pursuit of excitement and interest, we decided to go forth to do a basic forecasting model on COVID-19 dataset using fbprophet. . What and Why Prophet? . Prophet is Facebooks&#39;open source time series prediction. Prophet decomposes time series into trend, seasonality and holiday. It has intuitive hyper parameters which are easy to tune. . Prophet time series = Trend + Seasonality + Holiday + error . Trend models non periodic changes in the value of the time series. Seasonality is the periodic changes like daily, weekly, or yearly seasonality. Holiday effect which occur on irregular schedules over a day or a period of days. Error terms is what is not explained by the model. . We believe that the advantages of using Prophet are: . Accommodates seasonality with multiple periods | Prophet is resilient to missing values | Best way to handle outliers in Prophet is to remove them | Fitting of the model is fast | Intuitive hyper parameters which are easy to tune | . Credits to https://towardsdatascience.com/time-series-prediction-using-prophet-in-python-35d65f626236 for information on Prophet. . . . The performance of the model . . Part E . Creating a Dashbord for viewing COVID-19 Predictions . Our very own COVID-19 Forecasting Dashboard . Using the model that we built, we created a COVID-19 Forecasting Dashboard. You can view it in this link: . https://covid-prediction.herokuapp.com/ . Our main motivation here was to be able to learn how to best provide the model outcomes to audience. . You can see the code in our file under the name: Covid-pred . Conclusions . The data factors being used for calculating the Happiness Index of the countries is not holistic and inclusive. There are other factors to also be considered. GDP per capita seems to be a skewed figure itself and the limitations that GDP poses is highly likely to bias the happiness score. . | We did not find much correlation between no. of terror attacks and happiness index of a country. However, we believe we need to consider more factors &amp; influences pertaining to terrorism for us to properly see the relationship. . | For COVID-19 forecasts, we performed univariate analysis on our historical data, which made us realize that historical data alone might not be sufficient for the prediction. But certainly, this is one of the main predictors and it can be used with other set of predictors to create a more powerful model. . | . Improvements That Can Be Done . Improvement: Figure out another way to calculate Happiness Index of a country which includes more holistic and inclusive factors . Based on our observations, we believe that factors apart from 6 selected need to be considered in order to make accurate happiness index scoring. A possible improvement would be to research on an alternative way to calculate the index without using GDP per capita as a score . Improvement: To move into using NLP &amp; Decision Trees for analyzing Terrorism Data . Most of the factors in the Terrorism Dataset were text based. Hence, using NLP here will be best for us to understand the influences of the predictor on the response. To improve model prediction, we believe models pertaining to Decision Trees will help. . Improvement: To move into Multivariate Analysis . We forecasted COVID-19 cases using only past data – however, we are aware that historical data alone is not enough to make accurate forecasts. There are many other external factors – our intention was to more or less look at the trend and observe how this trend will move in the future. .",
            "url": "https://sakzsee.github.io/Sakthisree/2020/10/23/A-Country-Secret-to-Happiness.html",
            "relUrl": "/2020/10/23/A-Country-Secret-to-Happiness.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Distinguish Your Own Digits (DYOD)",
            "content": "Hola! . . As part of the Univ.ai KTF-1 course, I created this solution for one of the project questions. . Write a classifier that distinguishes the number 8 &amp; number 3. . Gee. This might seem like a really simple question as first. . &quot;Well, let&#39;s use scikit learn and all the other libraries we have and create a classifier in 3 lines.&quot; . But no. Let&#39;s use the Kudzu library - which I coded from scratch - where all the implementation for running a neural network is written from scratch. . Credits to Rahul (&amp; of course, Joe Grus) for wonderfully showing how simple a neural network can be coded. . Throughout this project, I make use of the Kudzu libray to run the models. . Towards the end, you can also see a visualization of how the computer &quot;splits&quot; 3 &amp; 8. . Some of the things I have altered in the Kudzu library:Added extra functions in the Callbacks call; Added tqdm for visualizing progress; Added softmax activation and softmax prime function. . Preparing the Data . &quot;What is MNIST anyway?&quot; . The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. . It was created by &quot;re-mixing&quot; the samples from NIST&#39;s original datasets. . The creators felt that since NIST&#39;s training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. . Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. . The MNIST database contains 60,000 training images and 10,000 testing images. . Half of the training set and half of the test set were taken from NIST&#39;s training dataset, while the other half of the training set and the other half of the test set were taken from NIST&#39;s testing dataset . Source:Wikipedia . Let&#39;s take a look at how this data looks like. . #collapse image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . . 2 . &lt;matplotlib.image.AxesImage at 0x1894e46f6c8&gt; . Filter data to get 3 and 8 out . Now in order to create our classifier, we must filter our 3s and 8s from the lot. . #filtering the 3 and 8 data - splitting it into test and train train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) y_train.shape, y_test.shape . ((11982, 1), (1984, 1)) . So how does this work? . In regression, we saw our model outputting numerical values. . But in the case of logistic regression, our model will instead be spewing out probability values of the given input belonging to a certain class. . This is based on the premise the the input space can in fact be seperated into two seperate &#39;regions&#39;(in the case of 2 classes) by a linear boundary. . Ideally, in the case of two dimensions, our linear boundary would be a line. For three dimensions, it would end up being a plane. For higher dimensions, we would call this boundary as a &#39;hyperplane&#39;. . $$ begin{eqnarray} y = 1 &amp; , , , &amp; {w} cdot{x} + b ge 0 y = 0 &amp; , , , &amp; {w} cdot{x} + b &lt; 0 end{eqnarray} $$Consider the sigmoid function: . $$h(z) = frac{1}{1 + e^{-z}}.$$ . with the identification: . $$z = {w} cdot{x} + b$$ . At $z=0$ this function has the value 0.5. . If $z &gt; 0$, $h &gt; 0.5$ and as $z to infty$, $h to 1$. . If $z &lt; 0$, $h &lt; 0.5$ and as $z to - infty$, $h to 0$. . Through Maximum Likelihood estimation, we can then arrive at our log likelihood. . $$L = P(y mid {x},{w}).$$ . $$l = log L = log(P(y mid {x},{w})).$$ . Thus . $$ begin{eqnarray} l &amp;=&amp; log left( prod_{y_i in cal{D}} h({w} cdot{x_i} + b)^{y_i} left(1 - h({w} cdot{x_i} + b) right)^{(1-y_i)} right) &amp;=&amp; sum_{y_i in cal{D}} log left(h({w} cdot{x_i} + b)^{y_i} left(1 - h({w} cdot{x_i} + b) right)^{(1-y_i)} right) &amp;=&amp; sum_{y_i in cal{D}} log ,h({w} cdot{x_i} + b)^{y_i} + log , left(1 - h({w} cdot{x_i} + b) right)^{(1-y_i)} &amp;=&amp; sum_{y_i in cal{D}} left ( y_i log(h({w} cdot{x_i} + b)) + ( 1 - y_i) log(1 - h({w} cdot{x_i} + b)) right ) end{eqnarray}$$The negative of this log likelihood (henceforth abbreviated NLL), is also called the Binary Cross-Entropy aka Negative Log likelihood. . $$ NLL = - sum_{y_i in cal{D}} left ( y_i log(h({w} cdot{x})) + ( 1 - y_i) log(1 - h({w} cdot{x})) right )$$ . Neural Network . For our neural network, we need to use this cost function in order to find the gradient and perform gradient descent. . The general sequence of how data is passed through a neural network layers. . These layers are mostly of the sequence: . Linear transformation or affine layer | Non-linear transformation or activation layer | Cost function calculation | This is known as forward propogation. . In the case of backward propogation, the cost function is used to calculate the gradient and find the direction in which to move towards the global minima of loss. . This is done by going in the reverse or backward phase - we now propagate the derivatives backward through the layer cake. . The weights are then updated through means of the derivatives calculated by the chain rule. . It is also important to note that a learning rate is initialized so as to control the &quot;length&quot; of the step towards the minima. . If the step is too long, you might end up jumping to the next mountain. . If your step is too short, you might end up camping at night for three days, almost getting mauled by bears, before you reach the minima. . 1. I use the following configuration (or similar) to set up the model for training. . class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 200 config.bs = 50 . Now construct a model which has the following layers . A first affine layer which has 784 inputs and does 100 affine transforms. These are followed by a Relu | A second affine layer which has 100 inputs from the 100 activations of the past layer, and does 100 affine transforms. These are followed by a Relu | A third affine layer which has 100 activations and does 2 affine transformations to create an embedding for visualization. There is no non-linearity here. | A final &quot;logistic regression&quot; which has an affine transform from 2 inputs to 1 output, which is squeezed through a sigmoid. | train_data = Data(X_train, y_train) loss = BCE() opt = GD(config.lr) sampler = Sampler(train_data, config.bs, shuffle=True) train_dl = Dataloader(train_data, sampler) . layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;relu&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;relu&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;output&quot;, 2, 1), Sigmoid(&quot;sigmoid&quot;)] model = Model(layers) . 2. Create a callback class . I further added code to the AccCallback class to add in the following functionalities. . Initialized it to have accuracy arrays | self.accuracies = [] self.test_accuracies = [] . Then at the end of each epoch, calculated the probabilities and hence predictions on both the training set and the test set. Printed these out once per epoch. . | Acumulated these in the above array. This will require me to keep track of all 4 training and test sets. . | I edited the Learner for this or pass these sets in some kind of data object to the callback. . | I also added tqdm functionality in the AccCallback class in order to visually see the batch running progress per epoch. . | learner = Learner(loss, model, opt, config.num_epochs) acc = AccCallback(learner, config.bs) learner.set_callbacks([acc]) . 3. Train the model . Now that the training has completed, let&#39;s take a look at how our model performed. . #collapse plt.figure(figsize=(14,7)) plt.plot(acc.accuracies, label = &quot;Training Accuracy&quot;, linewidth = &quot;3&quot;) plt.plot(acc.test_accuracies, label = &quot;Validation Accuracy&quot;, linewidth = &quot;3&quot;) plt.legend(loc=&#39;upper left&#39;) plt.grid(&#39;both&#39;, alpha = 0.95, linestyle=&quot;--&quot;) plt.title(&quot;Training Accuracy vs. Validation Accuracy&quot;); . . Not too bad! . Usually when the epochs progress, if the training accuracy and validation accuracy differ widely and move away from each other, then it is a sign of overfitting. . In our case, there is a small &quot;moving away&quot; happening. But since it is not so singificant, we are allowed to call it a good performance. . The intuition behind validation accuracy and training accuracy is that if your model ends up learning too well, then it wouldn&#39;t be able to generalize for any new data it sees. This is ,in fact, what we call overfitting. . #collapse plt.figure(figsize=(14,7)) plt.plot(acc.losses, linewidth = &quot;3&quot;, color = &quot;b&quot;) plt.grid(&#39;both&#39;, alpha = 0.95, linestyle=&quot;--&quot;) plt.title(&quot;Losses over Epochs&quot;); . . The graph above is the loss we see over the epochs. Clearly, the model is learning to approach the global minima through gradient descent. . (although it is important to note that we may not have reached it completely) . #collapse plt.figure(figsize=(14,7)) plt.plot(acc.batch_losses, color = &quot;r&quot;) plt.grid(&#39;both&#39;, alpha = 0.95, linestyle=&quot;--&quot;) plt.title(&quot;Batch Losses over Epochs&quot;); . . 4. Plot the results . Our accuracy is 99.2%. Nice! . In order to see this more clearly, tn the embedding space ( the inputs of the &quot;logistic regression&quot;) I am now plotting the data points by running them forward through the network. . By coloring coding them with their actual class and plotting the probability contours (these will all be lines in the embedding space as from here on, this is a logistic regression), we can bettersee the points stranded on the &quot;wrong&quot; side of the probability 1/2 line. . I have also plotted the predictions against the actual values showing where they are in the embedding space. . I have also tried doing a 3D plot to see the Sigma shape of the logistics working. (X - first output from the affine layer; Y- second output from the affine layer; Z - the probability of the X,Y data) . #collapse actual_inputs = np.concatenate((X_train, X_test)) actual_outputs = np.concatenate((y_train, y_test)) predicted_outputs = 1*(model(actual_inputs) &gt;= 0.5) . . #collapse print(&quot;Your model accuracy is &quot;+ str(np.mean(actual_outputs == predicted_outputs) * 100) +&#39;.&#39;) . . Your model accuracy is 99.10496921094085. . #collapse #hide #getting the last two layers for plotting prob_layer = [Affine(&quot;output&quot;, 2, 1), Sigmoid(&quot;sigmoid&quot;)] prob_model = Model(prob_layer) prob_model.layers[0].params[&#39;w&#39;] = model.layers[5].params[&#39;w&#39;] prob_model.layers[0].params[&#39;b&#39;] = model.layers[5].params[&#39;b&#39;] . . #collapse visualize_model_1 = Model(layers[:-2]) plot_testing = visualize_model_1(X_test) . . #collapse plt.figure(figsize=(14,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], c = y_test.ravel()); plt.title(&quot;Visualization of the two classes of data in 2D space&quot;); . . #collapse #this would be our probability layer (last two layers) model_prob = Model(layers[-2:]) . . #collapse xgrid = np.linspace(-4, 1, 100) ygrid = np.linspace(-7.5, 7.5, 100) xg, yg = np.meshgrid(xgrid, ygrid) # xg and yg are now both 100X100 -&gt; we need to conver them to single arrays xg_interim = np.ravel(xg) yg_interim = np.ravel(yg) # xg_interim, yg_interim are now arrays of len 10000, now we will stack them and then transpose to get desired shape of n rows, 2 columns X_interim = np.vstack((xg_interim, yg_interim)) X = X_interim.T # We want a shape of n rows and 2 columns in order to be able to feed this to last affine # This last affine takes only two columns, hence the above transformation probability_contour = model_prob(X).reshape(100,100) . . #collapse plt.figure(figsize=(14,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline = True ); plt.title(&quot;Probability Contour that distinguishes one data class from the other&quot;); . . #collapse import numpy as np from mpl_toolkits.mplot3d import Axes3D import pandas as pd import matplotlib.pyplot as plt import scipy import math from matplotlib import cm xx, yy = np.mgrid[-5:5:.01, -5:5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = prob_model(grid) #contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;, vmin=0, vmax=1) fig = plt.figure(figsize=(15, 10)) ax = plt.axes(projection=&#39;3d&#39;) ax.scatter(model_points_3, visualization_points_3.T[0], visualization_points_3.T[1],linewidth=0, c=&#39;r&#39;) ax.scatter(model_points_8, visualization_points_8.T[0], visualization_points_8.T[1],linewidth=0, c=&#39;b&#39;) plt.title(&quot;3D overview of Input data wrt to its Sigma value&quot;); #ax.plot_surface(xx, yy, z, alpha=0.2) #ax.view_init(elev=90, azim=75) . . Super cool! . Let&#39;s now do a simple Logistic Regression Model and compare it with the Neural Network we had constructed. . It would be pretty interesting to see the difference in outcomes. . #collapse plt.figure(figsize=(14,7)) plt.plot(acc.test_accuracies, &#39;g-&#39;, label = &quot;Val Accuracy - NN&quot;) plt.plot(acc.accuracies, &#39;r-&#39;, label = &quot;Training Accuracy - NN&quot;) plt.plot(acc2.test_accuracies, &#39;b-&#39;,linestyle=&quot;--&quot;, label = &quot;Val Accuracy - Logistic Reg&quot;) plt.plot(acc2.accuracies, &#39;c-&#39;,linestyle=&quot;--&quot;, label = &quot;Training Accuracy - Logistic Reg&quot;) plt.ylim(0.8,1) ## for a more spread out view plt.legend() plt.title(&quot;Comparing NN accuracies &amp; Logistic Regression Model accuracy&quot;); . . #collapse actual_inputs = np.concatenate((X_train, X_test)) actual_outputs = np.concatenate((y_train, y_test)) predicted_outputs = 1*(model_logistic(actual_inputs) &gt;= 0.5) print(&quot;Your model accuracy is &quot;+ str(np.mean(actual_outputs == predicted_outputs) * 100) +&#39;.&#39;) . . Your model accuracy is 96.28383216382643. . ...And the winner is our lovely Neural Network model! . Looking at this graph, clearly the Neural network performed better than our simple logistics model. . Beauty of Neural Networks . Now that we&#39;ve seen both these different types of models performing, you might have the question - why and how is it doing what it does? . Well, Neural networks are absolutely brilliant in the way they work - and brilliance doesn&#39;t really need to equate itself with being increasingly complex. With the right mindset and understanding, you will come to realize the beauty in the simplicity of how it works. . As a matter of fact, the Universal Approximation Theorem ,to me, is the theoretical foundation of why neural networks work. . It states that a neural network with one hidden layer containing a sufficient but finite number of neurons can approximate any continuous function to a reasonable accuracy, under certain conditions for activation functions (namely, that they must be sigmoid-like). . It was formulated in 1989 by George Cybenko only for sigmoid activations - however later on it was proven by Kurt Hornik in 1991 that it can be applied to all activation functions. . Hornik also came to realize that the architecture of the neural network, not the choice of function, was the key deciding factor for the performance- and this discovery was significant in sparking the exciting developement of neural networks into the variety of applications it does today. . I believe that understanding it might be a good step towards understanding truly how these networks work. . Without delving too much into the details and mathematics of it, the key point of how the Universal Approximation theory works is the following - . Instead of trying to create a complex function that maps your inputs to your outputs, divide your function into smaller peices and assign a neuron performing simple linear manipulations to each of these less complicated peices to represent it. . One of my favorite teachers is Grant from 3Blue1Brown and the one thing that stuck with me from his videos is that given any complex problem, always try to break it down to its most simplest form. . And this is what we are doing here! . One of the things to note however is that the Universal Approximation theorm isn&#39;t meant to generalize well. . Wait, what? . Yes - neural networks generally just pose as great approximators but if you provide it with a value outside the range of the inputs, it would most probably fail. . This is sorta similar to the limited Taylor series apprixmation that models a sine wave in a certain range but it goes bezerk ourside of it. . Ultimately, neural networks are really estimators that are able to solve multi-dimensional problems and this is quite impressive by itself. . I understand that there are a lot of perspectives and there&#39;s so much to explore! And that is exactly what makes this field exciting. . At the end, if there&#39;s one of many things humanity can be proud, then it&#39;s definately the fact that we taught our computers to distingush cats and dogs! .",
            "url": "https://sakzsee.github.io/Sakthisree/2020/08/09/LogisticsClassifier.html",
            "relUrl": "/2020/08/09/LogisticsClassifier.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "A Python Dashboard in the time of Corona",
            "content": "India . Last update: 2021-02-18 . Confirmed cases: . 10963259 (+13239) . Confirmed deaths: . 156150 (+100) . Recovered: . 10665921 (+10915) . State Confirmed Deceased Recovered PConfirmed PDeceased PRecovered Fatality Rate . 0 India | 10963259 | 156150 | 10665921 | 10950020 | 156050 | 10655006 | 1.420000 | . 1 Maharashtra | 2081520 | 51669 | 1987804 | 2076093 | 51631 | 1985261 | 2.480000 | . 2 Kerala | 1021433 | 4047 | 956935 | 1016849 | 4033 | 951742 | 0.400000 | . 3 Karnataka | 946860 | 12282 | 928767 | 946454 | 12276 | 928461 | 1.300000 | . 4 Andhra Pradesh | 889077 | 7166 | 881292 | 889010 | 7165 | 881238 | 0.810000 | . 5 Tamil Nadu | 846937 | 12444 | 830320 | 846480 | 12438 | 829850 | 1.470000 | . 6 Delhi | 637445 | 10896 | 625496 | 637315 | 10894 | 625343 | 1.710000 | . 7 Uttar Pradesh | 602555 | 8711 | 591270 | 602410 | 8707 | 591013 | 1.450000 | . 8 West Bengal | 573193 | 10239 | 559282 | 573012 | 10237 | 559039 | 1.790000 | . 9 Odisha | 336460 | 1966 | 333888 | 336397 | 1965 | 333788 | 0.580000 | . 10 Rajasthan | 319270 | 2783 | 315212 | 319166 | 2782 | 315135 | 0.870000 | . 11 Chhattisgarh | 310210 | 3790 | 303415 | 309934 | 3788 | 303137 | 1.220000 | . 12 Telangana | 297113 | 1622 | 293799 | 296950 | 1620 | 293698 | 0.550000 | . 13 Haryana | 269405 | 3042 | 265549 | 269322 | 3041 | 265455 | 1.130000 | . 14 Gujarat | 266297 | 4403 | 260198 | 266034 | 4403 | 259928 | 1.650000 | . 15 Bihar | 261942 | 1531 | 259898 | 261879 | 1529 | 259830 | 0.580000 | . 16 Madhya Pradesh | 258574 | 3844 | 252821 | 258333 | 3842 | 252628 | 1.490000 | . 17 Assam | 217344 | 1091 | 214639 | 217330 | 1090 | 214628 | 0.500000 | . 18 Punjab | 177376 | 5732 | 169002 | 177108 | 5722 | 168833 | 3.230000 | . 19 Jammu and Kashmir | 125634 | 1954 | 122992 | 125547 | 1951 | 122939 | 1.560000 | . 20 Jharkhand | 119477 | 1084 | 117926 | 119439 | 1084 | 117889 | 0.910000 | . 21 Uttarakhand | 97005 | 1684 | 93339 | 96964 | 1683 | 93309 | 1.740000 | . 22 Himachal Pradesh | 58312 | 981 | 57017 | 58296 | 981 | 56948 | 1.680000 | . 23 Goa | 54421 | 786 | 53125 | 54369 | 786 | 53074 | 1.440000 | . 24 Puducherry | 39526 | 660 | 38667 | 39506 | 660 | 38644 | 1.670000 | . 25 Tripura | 33354 | 388 | 32938 | 33352 | 388 | 32937 | 1.160000 | . 26 Manipur | 29225 | 373 | 28769 | 29223 | 373 | 28759 | 1.280000 | . 27 Chandigarh | 21352 | 348 | 20854 | 21322 | 348 | 20832 | 1.630000 | . 28 Arunachal Pradesh | 16836 | 56 | 16775 | 16833 | 56 | 16774 | 0.330000 | . 29 Meghalaya | 13946 | 148 | 13743 | 13945 | 148 | 13719 | 1.060000 | . 30 Nagaland | 12191 | 91 | 11919 | 12190 | 90 | 11913 | 0.750000 | . 31 Ladakh | 9783 | 130 | 9605 | 9782 | 130 | 9604 | 1.330000 | . 32 Sikkim | 6126 | 135 | 5836 | 6121 | 135 | 5835 | 2.200000 | . 33 Andaman and Nicobar Islands | 5014 | 62 | 4944 | 5014 | 62 | 4944 | 1.240000 | . 34 Mizoram | 4399 | 10 | 4372 | 4396 | 10 | 4369 | 0.230000 | . 35 Dadra and Nagar Haveli and Daman and Diu | 3384 | 2 | 3349 | 3384 | 2 | 3349 | 0.060000 | . 36 Lakshadweep | 263 | 0 | 164 | 261 | 0 | 161 | 0.000000 | .",
            "url": "https://sakzsee.github.io/Sakthisree/2020/08/08/COVID19-Dashboard.html",
            "relUrl": "/2020/08/08/COVID19-Dashboard.html",
            "date": " • Aug 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "If there was one word to describe myself, it would be the word “Wonderjunkie”. . I often find myself in awe of everything around me - constantly seeking to understand the world I am living in. . My innate sense of curiosity and the mental model of a polymath has always propelled me to keep learning as much as I can and grabbing every opportunity to do something worthwhile. . The endless possibilities of marvelous time-traveling machines and tech-utopian cities created a spark in me to become the tech-evangelist that I am today. . I am very passionate about Conservation-based tech in the fight against global warming and in creating an empowered community that no longer is victim to the trails of poorly developed economic systems. . I’m a strong believer of using one’s skills for good and help our world get past its adversities, and I don’t want to end in just believing, I want to do it. . I aim to use this space to embed my thoughts in 1s and 0s and capture my learning path throughout. .",
          "url": "https://sakzsee.github.io/Sakthisree/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sakzsee.github.io/Sakthisree/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}